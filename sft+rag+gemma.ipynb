{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb514e0-cbc8-4c88-85c4-dc10a04227c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "Gemma2ForCausalLM\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from accelerate import Accelerator\n",
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bd378-160a-4550-86fe-e093ad418072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #QLoRA\n",
    "# lora_config = LoraConfig(\n",
    "#     r=6,#멀티헤드어텐션 헤드 개수\n",
    "#     lora_alpha = 8, #어텐션 계수 스케일\n",
    "#     lora_dropout = 0.05, #드롭아웃 비율\n",
    "#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba56b9-fc3c-48f2-8a50-1a1ff1143008",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=None,\n",
    "    trust_remote_code=True,\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a652ce-12e9-4383-bf08-6356c000dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train.csv'\n",
    "train_data = pd.read_csv(file_path)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "val_data=train_data[:50]\n",
    "train_data = train_data[50:]\n",
    "\n",
    "val_label_df = val_data[['question', 'answer']]\n",
    "\n",
    "train_data[\"prompt\"] = (\n",
    "    \"<start_of_turn>user\\n\"\n",
    "    \"당신은 뉴스 기사를 바탕으로 질문에 대해 간결하고 정확하게 답변해주는 어시스턴트입니다.\\n\"\n",
    "    \"다음에 주어지는 기사 내용을 잘 읽고, 그에 대한 질문에 단답형으로 응답해주세요.\\n\\n\"\n",
    "    \"기사 내용:\\n\" + train_data['context'] + \"\\n\\n\"\n",
    "    \"질문:\\n\" + train_data['question'] + \"\\n\"\n",
    "    \"질문에 대한 답변은 핵심만 포함된 1~2단어 수준의 짧은 응답이면 됩니다.\\n\"\n",
    "    \"<end_of_turn>\\n\"\n",
    "    \"<start_of_turn>model\\n\"\n",
    "    \"Answer: \" + train_data['answer'] + \"\\n\"\n",
    "    \"<end_of_turn>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f5f36-916f-4272-a989-dd692db87c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_pandas(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0025c-745d-48a1-8023-0a8b2f8578e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8705de-3888-42c9-9fe6-48dc5dd646d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    return example['prompt']\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    max_seq_length=None,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs6\",\n",
    "        num_train_epochs = 2,\n",
    "        max_steps=-1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_steps=2,\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1000,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "        save_strategy='steps',\n",
    "        save_steps=100,\n",
    "        \n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b7be2-00ee-45ca-804b-e386c5a7d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter\"\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2667f-fb3b-4749-9a17-47906ee75bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cfde2-b16d-47a3-a89c-8ede4480f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e5d90-04a8-4ee4-a824-9055dc9593e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ab313-62c8-41f7-88a2-f7c2971f0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf61dfe-855e-4f1a-ac6f-5d3ea906674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_path = \"./model_3\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216128c2-1803-40b0-bd9f-0516a819b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec392c3-236b-497f-af7e-60bee5ad654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af71bb-a7a3-4136-89ea-4b2fbc8e3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d782f-7d00-4dbd-a851-efd694c877b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[\"prompt\"] = (\n",
    "    \"<start_of_turn>user\\n\"\n",
    "    + \"주어진 Context를 토대로 Question에 답변해\\n\"  \n",
    "    + \"Context: \" + val_data['context'] + \"\\n\"\n",
    "    + \"Question: \" + val_data['question'] + \"\\n\"\n",
    "    + \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    + \"Answer: \" + val_data['answer'] + \"\\n\"\n",
    "    + \"<end_of_turn>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811865d-bf5a-44e5-8730-619f399292d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0755a-56b1-4e49-8990-406bd0273b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93a6df-c2ec-4792-9b4e-f63455b40c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_pipeline(question_prompt, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a92fac-a262-4aac-b1b9-516469efdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e99b85-ff57-4eac-8e38-4ac1156320bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_(text):\n",
    "        ''' 불필요한 기호 제거 '''\n",
    "        text = re.sub(\"'\", \" \", text)\n",
    "        text = re.sub('\"', \" \", text)\n",
    "        text = re.sub('《', \" \", text)\n",
    "        text = re.sub('》', \" \", text)\n",
    "        text = re.sub('<', \" \", text)\n",
    "        text = re.sub('>', \" \", text)\n",
    "        text = re.sub('〈', \" \", text)\n",
    "        text = re.sub('〉', \" \", text)\n",
    "        text = re.sub(\"\\(\", \" \", text)\n",
    "        text = re.sub(\"\\)\", \" \", text)\n",
    "        text = re.sub(\"‘\", \" \", text)\n",
    "        text = re.sub(\"’\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        '''연속된 공백일 경우 하나의 공백으로 대체'''\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        '''구두점 제거'''\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        '''소문자 전환'''\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(remove_(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "\n",
    "    # 문자 단위로 f1-score를 계산 합니다.\n",
    "    prediction_Char = []\n",
    "    for tok in prediction_tokens:\n",
    "        now = [a for a in tok]\n",
    "        prediction_Char.extend(now)\n",
    "\n",
    "    ground_truth_Char = []\n",
    "    for tok in ground_truth_tokens:\n",
    "        now = [a for a in tok]\n",
    "        ground_truth_Char.extend(now)\n",
    "\n",
    "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(prediction_Char)\n",
    "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def evaluate(ground_truth_df, predictions_df):\n",
    "    predictions = dict(zip(predictions_df['question'], predictions_df['answer']))\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for index, row in ground_truth_df.iterrows():\n",
    "        question_text = row['question']\n",
    "        ground_truths = row['answer']\n",
    "        total += 1\n",
    "        if question_text not in predictions:\n",
    "            continue\n",
    "        prediction = predictions[question_text]\n",
    "        f1 = f1 + f1_score(prediction, ground_truths)\n",
    "\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57941f-aa5a-451d-aa1a-96c56011aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7c415-bcd9-49e0-b636-cea6f576b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 추론 후, 전처리를 수행한 뒤, 완성된 정답으로 반환합니다.\n",
    "def generate_response(question_prompt):\n",
    "    # 생성할 최대 토큰 수와, 답변 생성 수, 패딩 토큰의 idx를 지정하여 모델 파이프 라인을 설정하고, 답변을 생성합니다.\n",
    "    response = qa_pipeline(question_prompt, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)[0]['generated_text']\n",
    "    if \"Answer:\" in response:\n",
    "            # Answer: 이후에 생성된 토큰 들만을 답변으로 사용합니다.\n",
    "            response = response.split(\"Answer:\", 1)[1][:20]\n",
    "\n",
    "            # 토큰 반복 생성 및 노이즈 토큰 관련 처리\n",
    "            if \"Que\" in response:\n",
    "                response = response.split(\"Que\", 1)[0]\n",
    "            if \"⊙\" in response:\n",
    "                response = response.split(\"⊙\", 1)[0]\n",
    "            if \"Con\" in response:\n",
    "                response = response.split(\"Con\", 1)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc741c05-d8f7-4bec-90bc-266ec8c429e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dict = {}\n",
    "count = 0\n",
    "\n",
    "for index, row in val_data.iterrows():\n",
    "    try:\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "\n",
    "\n",
    "        if context is not None and question is not None:\n",
    "            # question_prompt = f\"너는 주어진 Context를 토대로 Question에 답하는 챗봇이야. \\\n",
    "            #                     Question에 대한 답변만 있는 한 단어로 최대한 간결하게 답변하도록 해. \\\n",
    "            #                     # Context: {context} Question: {question}\\n Answer:\"\n",
    "            question_prompt =f\"당신은 주어진 Context를 기반으로 Question에 답하는 챗봇입니다.\\\n",
    "답변을 할 때, 반드시 context를 참고하세요.\\\n",
    " 문장이 아닌 간결한 단답이어야 합니다.\\\n",
    "Context: {context} Question: {question}\\n Answer:\"\n",
    "#             question_prompt=f\"당신은 주어진 Context를 기반으로 Question에 답하는 챗봇입니다.\\\n",
    "# 답변을 할 때, 반드시 context를 참고하세요.\\\n",
    "# 문장이 아닌 간결한 단답이어야 합니다.\\\n",
    "# 이 답변은 저의 대학 입시에 매우 중요합니다. 저를 위해 꼭 정답을 찾아주세요.\\\n",
    "# Context: {context} Question: {question}\\n Answer:\"\n",
    "            # question_prompt = f\"Context를 바탕으로 Question에 단답형으로 답변해 주세요.\\\n",
    "            #                     답변은 꼭 Context에 있는 내용이어야 해. \\\n",
    "            #                      Context: {context} Question: {question}\\n Answer:\"\n",
    "            # question_prompt = f\"주어진 Context를 기반으로 Question에 대해 간결하게, 한 단어로 답변하는 챗봇입니다. \\\n",
    "            #                      Context: {context} Question: {question}\\n Answer:\"\n",
    "    \n",
    "            answer = generate_response(question_prompt)\n",
    "            predict_dict[question] = answer\n",
    "        else:\n",
    "            predict_dict[question] = 'Invalid question or context'\n",
    "\n",
    "        print(\"Answer for question:\", question, \":\", predict_dict[question])\n",
    "        count += 1\n",
    "        print(\"Processed count:\", count)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {e}\")\n",
    "val_inf_df = pd.DataFrame(list(predict_dict.items()), columns=['question', 'answer'])\n",
    "val_inf_df.head()\n",
    "# f1-score 를 출력합니다.\n",
    "results = evaluate(val_inf_df, val_label_df)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775beb44-0625-4a75-9552-855036188bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'prj_data/test.csv'\n",
    "test_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ab59d-bba7-480d-99cb-1cfa32909360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "submission_dict = {}\n",
    "\n",
    "# 문서 쪼개기 및 벡터화 - 전체 test context에 대해 사전 구축\n",
    "docs = text_splitter.create_documents(test_data['context'].dropna().tolist())\n",
    "vectordb = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# QA 체인 구성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "# 추론 루프\n",
    "for index, row in test_data.iterrows():\n",
    "    try:\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        qid = row['id']\n",
    "\n",
    "        if context is not None and question is not None:\n",
    "            # 프롬프트 구성\n",
    "            question_prompt = (\n",
    "                \"<start_of_turn>user\\n\"\n",
    "                \"당신은 뉴스 기사를 바탕으로 질문에 대해 간결하고 정확하게 답변해주는 어시스턴트입니다.\\n\"\n",
    "                \"다음에 주어지는 기사 내용을 잘 읽고, 그에 대한 질문에 단답형으로 응답해주세요.\\n\\n\"\n",
    "                \"기사 내용:\\n\" + context + \"\\n\\n\"\n",
    "                \"질문:\\n\" + question + \"\\n\"\n",
    "                \"질문에 대한 답변은 핵심만 포함된 1~2단어 수준의 짧은 응답이면 됩니다.\\n\"\n",
    "                \"<end_of_turn>\\n\"\n",
    "                \"<start_of_turn>model\\n\"\n",
    "            )\n",
    "\n",
    "            # RAG를 활용한 답변 생성\n",
    "            response = qa_chain({\"query\": question_prompt})\n",
    "            answer = response[\"result\"].strip()\n",
    "            submission_dict[qid] = answer\n",
    "        else:\n",
    "            submission_dict[qid] = 'Invalid question or context'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {index}, id={qid}: {e}\")\n",
    "        submission_dict[qid] = 'Error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12a187-1df6-4d93-8d03-82ccfa2d328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(submission_dict.items()), columns=['id', 'answer'])\n",
    "df.to_csv( './submission2.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c4ad6-ccf5-4530-aa5c-b01ef75d48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1=val_data['context'][0]\n",
    "n2=val_data['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f2ade-2bf4-4a1a-9799-c0f84f14d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{}를 토대로 {}에 답변해주세요.:\\n\\n{}\".format(n1, n2)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b483e-b598-43cd-85a8-c7a7a95763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2028e4c-7820-44de-9aff-9be7bc51d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_pipeline(question_prompt, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895789ca-f788-4513-943e-2fe2830a24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0375d3-9e8c-4797-8bb5-479562eb105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe_finetuned(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe353313-5b3a-4d91-a406-1ce5c4fb72fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645bef7-91fc-46d4-91a9-d66f62833756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f53cff-3f7b-4c25-9cf4-e84aa9245923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(submission_dict.items()), columns=['id', 'answer'])\n",
    "df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\n', '', x))\n",
    "df.to_csv( './submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
